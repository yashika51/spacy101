{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports language of your choice, I am starting with english\n",
    "import spacy\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making an nlp object\n",
    "nlp=English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc will be used to acess multiple attributes of nlp\n",
    "sent=[[\"Howdy Stranger, where have you been?\"],[\"My day was rough today, I was taken down by my friends and it was terrible\"],[\"I also bought a pair of blue glasses but they delivered pink instead and now I am no one but a mermaid\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(*sent[0])\n",
    "# print(*sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Howdy Stranger, where have you been?'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.de import German\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Liebe Grüße!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp1=German()\n",
    "doc1=nlp1('Liebe Grüße!')\n",
    "doc1.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When you call nlp on a string, spaCy first tokenizes the text and creates a document object.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can access tokens via doc by using indices. As nlp already tokenized it for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "you"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Howdy\n",
      "-----\n",
      "Stranger\n",
      "-----\n",
      ",\n",
      "-----\n",
      "where\n",
      "-----\n",
      "have\n",
      "-----\n",
      "you\n",
      "-----\n",
      "been\n",
      "-----\n",
      "?\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for i in doc:\n",
    "    print(i)\n",
    "    print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we slice the doc then the output received is in form of spans.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stranger, where\n"
     ]
    }
   ],
   "source": [
    "doc=doc[1:4]\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, in)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#like_num attribute and tok.i\n",
    "doc3=nlp('Would you find a number 5 in the sentence?')\n",
    "[(tok,doc3[tok.i+1]) for tok in doc3 if tok.like_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical models enable spaCy to make predictions in context. This usually includes part-of speech tags, syntactic dependencies and named entities.\n",
    "\n",
    "Models are trained on large datasets of labeled example texts.\n",
    "\n",
    "They can be updated with more examples to fine-tune their predictions – for example, to perform better on your specific data.\n",
    "\n",
    " \"en_core_web_sm\" package is a small English model that supports all core capabilities and is trained on web text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when spacy.load('model') gives error this is the right way of dodging the bullet\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded the english model and created an object nlp let's get started with more cool stuff.\n",
    "\n",
    "For each token in the doc, we can print the text and the .pos_ attribute, the predicted part-of-speech tag.\n",
    "\n",
    "In spaCy, attributes that return strings usually end with an underscore – attributes without the underscore return an integer ID value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4=nlp('I was eating a cookie I baked in quarantine and it tasted terrible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "was VERB\n",
      "eating VERB\n",
      "a DET\n",
      "cookie NOUN\n",
      "I PRON\n",
      "baked VERB\n",
      "in ADP\n",
      "quarantine NOUN\n",
      "and CCONJ\n",
      "it PRON\n",
      "tasted VERB\n",
      "terrible ADJ\n"
     ]
    }
   ],
   "source": [
    "for tok in doc4:\n",
    "    print(tok,tok.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the part-of-speech tags, we can also predict how the words are related. For example, whether a word is the subject of the sentence or an object.\n",
    "\n",
    "The .dep_ attribute returns the predicted dependency label.\n",
    "\n",
    "The .head attribute returns the syntactic head token. You can also think of it as the parent token this word is attached to.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I nsubj\n",
      "was aux\n",
      "eating ROOT\n",
      "a det\n",
      "cookie dobj\n",
      "I nsubj\n",
      "baked relcl\n",
      "in prep\n",
      "quarantine pobj\n",
      "and cc\n",
      "it nsubj\n",
      "tasted conj\n",
      "terrible acomp\n"
     ]
    }
   ],
   "source": [
    "for tok in doc4:\n",
    "    print(tok,tok.dep_)\n",
    "#     print(tok.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How about plotting these dependencies? :hearty eyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting msgpack-numpy<0.4.4.0\n",
      "  Downloading msgpack_numpy-0.4.3.2-py2.py3-none-any.whl (5.2 kB)\n",
      "Requirement already satisfied: msgpack>=0.3.0 in d:\\yashika sharma\\anaconda3\\envs\\python\\lib\\site-packages (from msgpack-numpy<0.4.4.0) (0.5.6)\n",
      "Requirement already satisfied: numpy>=1.9.0 in d:\\yashika sharma\\anaconda3\\envs\\python\\lib\\site-packages (from msgpack-numpy<0.4.4.0) (1.18.1)\n",
      "Installing collected packages: msgpack-numpy\n",
      "  Attempting uninstall: msgpack-numpy\n",
      "    Found existing installation: msgpack-numpy 0.4.4.3\n",
      "    Uninstalling msgpack-numpy-0.4.4.3:\n",
      "      Successfully uninstalled msgpack-numpy-0.4.4.3\n",
      "Successfully installed msgpack-numpy-0.4.3.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install displacy\n",
    "!pip install \"msgpack-numpy<0.4.4.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Gives Error in local Jupyter notebook but works in colab\n",
    "# displacy.render(doc4, style='dep', jupyter=True, options={'distance': 90})\n",
    "\n",
    "# displacy.serve(doc4)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc1 = nlp(\"This is a sentence.\")\n",
    "doc2 = nlp(\"This is another sentence.\")\n",
    "displacy.serve([doc1, doc2], style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Serving on port 5000...\u001b[0m\n",
      "    Using the 'ent' visualizer\n",
      "\n",
      "\n",
      "    Shutting down server on port 5000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.\"\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "displacy.serve(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER\n",
    "\n",
    "These are the attributes:\n",
    "\n",
    "- Text: The original entity text.\n",
    "- Start: Index of start of entity in the Doc. Use ent.start_char\n",
    "- End: Index of end of entity in the Doc. Use ent.end_char\n",
    "- Label: Entity label, i.e. type. Use ent.label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Darshna ORG\n"
     ]
    }
   ],
   "source": [
    "#Entities and Labels\n",
    "\n",
    "for i in doc.ents:\n",
    "    print(i.text,i.label_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Darshna 88 95\n"
     ]
    }
   ],
   "source": [
    "for i in doc.ents:\n",
    "    print(i.text,i.start_char,i.end_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick tip: To get definitions for the most common tags and labels, you can use the spacy.explain helper function.\n",
    "\n",
    "For example, \"GPE\" for geopolitical entity isn't exactly intuitive – but spacy.explain can tell you that it refers to countries, cities and states.\n",
    "\n",
    "The same works for part-of-speech tags and dependency labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain(\"GPE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule Based Matching\n",
    "\n",
    "\n",
    "The SpaCy's matcher works similar to Regex but is better in terms of working with doc and tok objects as well and not just strings.\n",
    "\n",
    "We can search for both text and lexical attributeds. The model's prediction can be used to define the rules.\n",
    "\n",
    "Let's say we want to use 'Love' only if it's noun and not verb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matcher pattern is a dictionary with keys=token attribute and value=expexted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load a model and create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat1=[{'TEXT':'Love'},{'TEXT':'Hard'}]\n",
    "pat2=[{'LOWER':'enemy'},{'LOWER':'closer'}]\n",
    "pat3=[{'LEMMA':'eat'},{'POS':'NOUN'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1=\"It's hard to love someone but every song says Love Hard\"\n",
    "text2=\"Keep your frns close but enemies closer\"\n",
    "text3=\"I dunno how to eat with chopsticks but eating donuts is easier\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use matcher.add() function to add the pattern.\n",
    "\n",
    "- The first argument is a unique ID to identify which pattern was matched. \n",
    "- The second argument is an optional callback. We don't need one here, so we set it to None. \n",
    "- The third argument is the pattern.\n",
    "\n",
    "To match the pattern on a text, we can call the matcher on any doc.\n",
    "\n",
    "This will return the matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add('love',None,pat1)\n",
    "doc1=nlp(text1)\n",
    "matches=matcher(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9223393045180330578, 0, 1),\n",
       " (9223397443226579542, 0, 1),\n",
       " (9223393045180330578, 1, 2),\n",
       " (9223397443226579542, 1, 2),\n",
       " (9223393045180330578, 2, 3),\n",
       " (9223397443226579542, 2, 3),\n",
       " (9223393045180330578, 3, 4),\n",
       " (9223397443226579542, 3, 4),\n",
       " (9223393045180330578, 4, 5),\n",
       " (9223397443226579542, 4, 5),\n",
       " (9223393045180330578, 5, 6),\n",
       " (9223397443226579542, 5, 6),\n",
       " (9223393045180330578, 6, 7),\n",
       " (9223397443226579542, 6, 7),\n",
       " (9223393045180330578, 7, 8),\n",
       " (9223397443226579542, 7, 8),\n",
       " (9223393045180330578, 8, 9),\n",
       " (9223397443226579542, 8, 9),\n",
       " (9223393045180330578, 9, 10),\n",
       " (9223397443226579542, 9, 10),\n",
       " (9223393045180330578, 10, 11),\n",
       " (9223397443226579542, 10, 11),\n",
       " (9223393045180330578, 11, 12),\n",
       " (9223397443226579542, 11, 12)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ew! What does that mean?\n",
    "\n",
    "The matcher returned the match_id, start index and end index. Loop game will make this easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It\n",
      "It\n",
      "'s\n",
      "'s\n",
      "hard\n",
      "hard\n",
      "to\n",
      "to\n",
      "love\n",
      "love\n",
      "someone\n",
      "someone\n",
      "but\n",
      "but\n",
      "every\n",
      "every\n",
      "song\n",
      "song\n",
      "says\n",
      "says\n",
      "Love\n",
      "Love\n",
      "Hard\n",
      "Hard\n"
     ]
    }
   ],
   "source": [
    "for match_id,start,end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc1[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating donuts\n"
     ]
    }
   ],
   "source": [
    "matcher.add('eat',None,pat3)\n",
    "doc3=nlp(text3)\n",
    "matches=matcher(doc3)\n",
    "for match_id,start,end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc3[start:end]\n",
    "    print(matched_span.text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pattern can look like this as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {\"IS_DIGIT\": True},\n",
    "    {\"LOWER\": \"fifa\"},\n",
    "    {\"LOWER\": \"world\"},\n",
    "    {\"LOWER\": \"cup\"},\n",
    "    {\"IS_PUNCT\": True}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example\tDescription\n",
    "- {\"OP\": \"!\"}\tNegation: match 0 times\n",
    "- {\"OP\": \"?\"}\tOptional: match 0 or 1 times\n",
    "- {\"OP\": \"+\"}\tMatch 1 or more times\n",
    "- {\"OP\": \"*\"}\tMatch 0 or more times\n",
    "\n",
    "\"OP\" can have one of four values:\n",
    "\n",
    "An \"!\" negates the token, so it's matched 0 times.\n",
    "\n",
    "A \"?\" makes the token optional, and matches it 0 or 1 times.\n",
    "\n",
    "A \"+\" matches a token 1 or more times.\n",
    "\n",
    "And finally, an \"*\" matches 0 or more times.\n",
    "\n",
    "Operators can make your patterns a lot more powerful, but they also add more complexity – so use them wisely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {\"LEMMA\": \"buy\"},\n",
    "    {\"POS\": \"DET\", \"OP\": \"?\"},  # optional: match 0 or 1 times\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
